name: test-dbt-models

# This workflow is not scheduled or run on PRs because it is manually triggered
# by the Data Department's Spark data extraction process upon completion. See
# https://github.com/ccao-data/service-spark-iasworld
on:
  workflow_dispatch:
    inputs:
      send_test_failure_notifications:
        description: "Whether to notify SNS topic subscribers about failures"
        required: false
        type: boolean

jobs:
  test-dbt-models:
    runs-on: ubuntu-latest
    # These permissions are needed to interact with GitHub's OIDC Token endpoint
    # so that we can authenticate with AWS
    permissions:
      id-token: write
      contents: read
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Setup dbt
        uses: ./.github/actions/setup_dbt
        with:
          role-to-assume: ${{ secrets.AWS_IAM_ROLE_TO_ASSUME_ARN }}

      - name: Install Python requirements
        run: pip install -r scripts/requirements.run_iasworld_data_tests.txt
        working-directory: ${{ env.PROJECT_DIR }}
        shell: bash

      - name: Run tests
        run: |
          python3 scripts/run_iasworld_data_tests.py \
            --target "$TARGET" \
            --output-dir ./qc_test_results/ \
            --vars "{\"data_test_iasworld_commercial_sns_topic\": \"$COMMERCIAL_SNS_TOPIC\"}"
        working-directory: ${{ env.PROJECT_DIR }}
        shell: bash
        env:
          USER: ${{ github.triggering_actor }}
          GIT_SHA: ${{ github.sha }}
          GIT_REF: ${{ github.ref_name }}
          GIT_AUTHOR: ${{ github.event.commits[0].author.name }}
          COMMERCIAL_SNS_TOPIC: ${{ secrets.AWS_SNS_COMMERCIAL_FAILURE_TOPIC_ARN }}

      - name: Save test results to S3
        run: |
          s3_prefix="s3://ccao-data-warehouse-us-east-1/qc"
          local_prefix="qc_test_results/metadata"
          for dir in "test_run" "test_run_result" "test_run_failing_row"; do
            echo "Copying ${dir} metadata to S3"
            aws s3 sync "${local_prefix}/${dir}" "${s3_prefix}/${dir}"
          done

          crawler_name="ccao-data-warehouse-qc-crawler"
          aws glue start-crawler --name "$crawler_name"
          echo "Triggered Glue crawler $crawler_name"
        working-directory: ${{ env.PROJECT_DIR }}
        shell: bash

      - name: Get current time
        if: failure()
        run: echo "TIMESTAMP=$(date -u +"%Y-%m-%dT%H:%M:%S")" >> "$GITHUB_ENV"
        shell: bash

      # Only triggered when called by a bot. Otherwise, notifications
      # go to whoever called the workflow
      - name: Send pipeline error notification
        if: github.event_name == 'workflow_dispatch' && github.triggering_actor == 'sqoop-bot[bot]' && failure()
        uses: ./.github/actions/publish_sns_topic
        with:
          sns_topic_arn: ${{ secrets.AWS_SNS_NOTIFICATION_TOPIC_ARN }}
          subject: "dbt tests failed for workflow run: ${{ github.run_id }}"
          body: |
            dbt tests failed for workflow ${{ github.run_id }}, run on ${{ env.TIMESTAMP }} UTC

            Link to failing workflow:
            https://github.com/${{ github.repository }}/actions/runs/${{ github.run_id }}

      - name: Send data test failure notifications
        if: inputs.send_test_failure_notifications
        run: |
          failure_notifications_file="$PROJECT_DIR/qc_test_results/failure_notifications.json"
          if [ -f "$failure_notifications_file" ]; then
            echo "Sending failure notifications"
            python3 ./.github/scripts/publish_dbt_test_failure_notifications.py \
              "$failure_notifications_file"
          else
            echo "No failure notifications need to be sent"
          fi
        shell: bash
