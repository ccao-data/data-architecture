name: test-dbt-models

# This workflow is not scheduled or run on PRs because it is manually triggered
# by the Data Department's sqoop data extraction process upon completion. See
# https://github.com/ccao-data/service-sqoop-iasworld
on: workflow_dispatch

jobs:
  test-dbt-models:
    runs-on: ubuntu-latest
    # These permissions are needed to interact with GitHub's OIDC Token endpoint
    # so that we can authenticate with AWS
    permissions:
      id-token: write
      contents: read
    steps:
      - name: Checkout
        uses: actions/checkout@v3

      - name: Setup dbt
        uses: ./.github/actions/setup_dbt
        with:
          role-to-assume: ${{ secrets.AWS_IAM_ROLE_TO_ASSUME_ARN }}

      - name: Run tests
        run: |
          # dbt doesn't differentiate between test failures and errors, but we
          # need to since we expect errors. Do this by capturing the output
          # and checking its contents to look for errors
          # TODO: Switch this back to the full set of tests once testing is
          # done
          if output=$(dbt test --target "$TARGET" --select iasworld_asmt_all_class_matches_pardat_class); then
            echo "$output"
          else
            status_code="$?"
            if [[ "$output" =~ "Runtime Error" || "$output" =~ "Compilation Error" ]]; then
              # The presence of an error string indicates that these tests
              # failed due to an error, so print the output and fail the
              # pipeline
              echo "$output"
              exit "$status_code"
            else
              # The tests must have failed rather than errored out, so
              # print the output but don't raise an error
              echo "$output"
            fi
          fi
        working-directory: ${{ env.PROJECT_DIR }}
        shell: bash

      - name: Save test results to S3
        run: |
          run_results="target/run_results.json"
          run_id=$(jq -r ".metadata.invocation_id" "$run_results")
          run_dt=$(jq -r ".metadata.generated_at" "$run_results")
          run_date=$(date -d "$run_dt" +%Y-%m-%d)

          output_file="target/run_results.parquet"
          python3 ./.github/scripts/format_dbt_run_results_as_parquet.py "$run_results"  > "$output_file"

          s3_path="s3://ccao-data-warehouse-us-east-1/qc/test_result/run_date=$run_date/run_id=$run_id/part-0.parquet"
          aws s3 cp "$output_file" "$s3_path"
          echo "Uploaded run results to $s3_path"

          crawler_name="ccao-data-warehouse-qc-crawler"
          aws glue start-crawler --name "$crawler_name"
          echo "Triggered Glue crawler $crawler_name"

      - name: Get current time
        if: failure()
        run: echo "TIMESTAMP=$(date -u +"%Y-%m-%dT%H:%M:%S")" >> "$GITHUB_ENV"
        shell: bash

      # Only triggered when called by a bot. Otherwise, notifications
      # go to whoever called the workflow
      - name: Send failure notification
        if: github.event_name == 'workflow_dispatch' && github.triggering_actor == 'sqoop-bot[bot]' && failure()
        uses: ./.github/actions/publish_sns_topic
        with:
          sns_topic_arn: ${{ secrets.AWS_SNS_NOTIFICATION_TOPIC_ARN }}
          subject: "dbt tests failed for workflow run: ${{ github.run_id }}"
          body: |
            dbt tests failed for workflow ${{ github.run_id }}, run on ${{ env.TIMESTAMP }} UTC

            Link to failing workflow:
            https://github.com/${{ github.repository }}/actions/runs/${{ github.run_id }}
