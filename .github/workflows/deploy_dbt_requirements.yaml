name: deploy-dbt-requirements

on:
  push:
    branches: [master]
  pull_request:
    branches: [master]
  workflow_dispatch:

jobs:
  deploy-dbt-requirements:
    runs-on: ubuntu-latest
    # These permissions are needed to interact with GitHub's OIDC Token endpoint
    # so that we can authenticate with AWS
    permissions:
      id-token: write
      contents: read
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Setup dbt
        uses: ./.github/actions/setup_dbt
        with:
          role-to-assume: ${{ secrets.AWS_IAM_ROLE_TO_ASSUME_ARN }}
          role-duration-seconds: 900

      - name: Parse requirements from dbt DAG
        run: |
          # Compile the DAG
          dbt compile -t "$TARGET"

          # Extract config.packages attributes from models
          packages_json=$(jq '
            .nodes
            | with_entries(
                select(
                  .value.config.packages != null and
                  (.value.config.packages | length) > 0
                )
              )
            | with_entries(
                .value = .value.config.packages
              )
          ' target/manifest.json)

          # Use jq to iterate over each key-value pair representing packages
          echo "$packages_json" | \
            jq -rc 'to_entries[] | {model_name: .key, requirements: .value}' | \
            while read -r item;
          do
            # Extract the key and value
            model_name=$(echo "$item" | jq -r '.model_name')
            requirements=$(echo "$item" | jq -r '.requirements[]')

            # Split the key by '.' and take the last two elements
            model_identifier=$(echo "$model_name" | awk -F. '{print $(NF-1)"."$NF}')

            # Define the filename
            filename="${model_identifier}.requirements.txt"

            # Create the file and write the contents
            echo "$requirements" | tr ' ' '\n' > "$filename"

            echo "File $filename created with contents:"
            cat "$filename"
            echo
          done
        working-directory: ${{ env.PROJECT_DIR }}
        shell: bash

      - name: Download, install, and bundle requirements from remote repositories
        run: |
          for requirements_file in *.requirements.txt; do
            # Extract the base name without extension and use it to define
            # filenames for some artifacts
            base_name=$(basename "$requirements_file" .requirements.txt)
            venv_name="${base_name}.venv"
            subdirectory_name="${base_name}/"
            zip_archive_name="${base_name}.requirements.zip"

            # Create and activate a Python virtual environment for requirements
            python3 -m venv "$venv_name"
            source "${venv_name}/bin/activate"

            # Install requirements into a subdirectory
            mkdir -p "$subdirectory_name"
            pip install -t "$subdirectory_name" -r "$requirements_file"

            # Create a zip archive from the contents of the subdirectory
            zip -r "$zip_archive_name" "$subdirectory_name"

            # Cleanup the intermediate artifacts
            deactivate
            rm -rf "$venv_name"
            rm -rf "$subdirectory_name"

            echo "Created zip archive $zip_archive_name from $requirements_file"
          done
        working-directory: ${{ env.PROJECT_DIR }}
        shell: bash

      - name: Push requirements to S3
        run: |
          for zipfile in *.zip; do
            aws s3 cp "$zipfile" "s3://ccao-dbt-athena-dev-us-east-1/packages/"
        working-directory: ${{ env.PROJECT_DIR }}
        shell: bash
